---
title: "Que comience el periplo de la Modelización"
format:
  revealjs:
    incremental: False  
    scrollable: true
    transition: slide
editor: 
  markdown: 
    wrap: 72
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse)
```

# 1. Bases de modelización

## Tener las cosas claras antes de empezar

1.  Tener clara la pregunta
2.  Tener una idea de la complejidad de la pregunta.
3.  Tener más o menos claras las variables que interfieren en el proceso.
4.  Tener claro **las técnicas y el flujo de trabajo necesarios para responder**


## Bases de modelización

![Machine learning: el sentimiento cuando te asomas por primera vez](Imagenes/60860fbc7d3ee.jpeg)

## Flujo completo de modelización

```{mermaid, height=800, width=800}
%%| fig-width: 6
%%| fig-cap: |
%%|   Flujo completo de un proceso de modelización con machine learning. 


flowchart TD
    A[Pregunta / hipótesis] -->|Conseguir Datos| B[Datos]
    B --> C[Datos: Entrenamiento]
    B --> F[Datos: Evaluación]
    C --> D[Datos: Aprendizaje]
    C -->|Conjunto de validación| E[Datos: Validación]
    D -->|Pre procesamiento| G[Entrenamiento del modelo]
    E -->|HiperParámetros| G
    G -->|Métricas Entrenamiento| H[Entrenamiento Seleccionado]
    H --> F
    F -->|Métricas Testing| I[Modelo Final]
    I -.-> |No satisfactorio| G
    I -->|Cloud Services| J[Modelo en producción]
    H -.->|Re-procesamiento| G
```


# 2. Validación cruzada

## Concepto

- **Objetivo**: Evaluar la capacidad de generalización de un modelo a datos no vistos.
- **Técnica**: Dividir el conjunto de datos en subconjuntos para entrenamiento y validación.

Repites este proceso varias veces, dividiendo tus datos de diferentes maneras. Esto ayuda a asegurar que tu evaluación del modelo no dependa de una sola división de datos.

## Leave-One-Out Cross-Validation (LOOCV)

- **Descripción**: Cada observación se usa como conjunto de validación una vez, y el resto como conjunto de entrenamiento.
- **Ventajas**:
  + Utiliza al máximo los datos para el entrenamiento.
  + Minimiza el sesgo de estimación.
- **Desventajas**:
  + Extremadamente costosa computacionalmente.
  + Alta variabilidad en las estimaciones. (necesario plotear la distribición)

## Leave-One-Out Cross-Validation (LOOCV)

![](Imagenes/LOOCV.png)


## K-Fold Cross-Validation

- **Descripción**: Divide los datos en K particiones. Realiza K iteraciones, cada vez usando una partición diferente como conjunto de validación.
- **Ventajas**:
  + Utiliza todos los datos para entrenamiento y validación.
  + Proporciona una estimación robusta del rendimiento del modelo.
- **Desventajas**:
  + Computacionalmente costosa si K es grande.

## K-Fold Cross-Validation

![](Imagenes/kfold_cross_validation.png)


## Bootstrap 

- **Descripción**: Genera múltiples muestras aleatorias con reemplazo a partir del conjunto de datos original. 

- **Ventajas**:
  + Proporciona una medida de la incertidumbre en la evaluación del modelo.
  + Útil para construir intervalos de confianza alrededor de las métricas de evaluación.
- **Desventajas**:
  + No es una técnica de validación cruzada en sí misma, sino más bien una técnica de remuestreo que se usa en conjunto con otras técnicas de validación cruzada.

## Bootstrap 

![](Imagenes/bootstraps.jpg)


## Shuffle-Split (Monte Carlo)


- **Descripción**: Se realizan múltiples divisiones aleatorias del conjunto de datos. En cada iteración, una fracción aleatoria de los datos se usa para entrenamiento y la fracción complementaria para validación.

- Ventajas:
  + Más flexibilidad en la proporción de entrenamiento/validación.
  + Menor sesgo debido a la aleatorización repetida.
- Desventajas:
  + No garantiza que todas las observaciones se utilicen en las validaciones.


## Advertencias sobre Validación Cruzada

**1. La elección del número de divisiones no debe ser aleatoria**

- Elegir un número adecuado de divisiones para evitar sesgos en la evaluación del modelo.
- Por ejemplo, en la validación cruzada de K-fold, es común utilizar valores típicos como 5 o 10 para K. Sin embargo, la elección de K debe considerar el tamaño del conjunto de datos y el equilibrio entre la varianza y el sesgo.

## Advertencias sobre Validación Cruzada

**2. Siempre estratifica en problemas de clasificación**

- Estratificar asegura una distribución equitativa de las clases en cada división.
- En problemas de clasificación, es esencial estratificar las divisiones para garantizar que todas las clases estén representadas en cada conjunto de entrenamiento y prueba. Esto ayuda a evitar sesgos y garantiza una evaluación más precisa del rendimiento del modelo.


## Advertencias sobre Validación Cruzada

**3. Elección de validación cruzada adecuada para un problema de regresión**

- Aunque la validación cruzada de K-fold es comúnmente utilizada en problemas de regresión, también es importante considerar otras técnicas, como la validación cruzada estratificada, especialmente cuando hay una distribución desigual de los datos o se necesita una evaluación más precisa del rendimiento del modelo.

## Advertencias sobre Validación Cruzada

**4. Nunca Sobremuestrear antes de la validación cruzada.**

- La sobremuestreo antes de la validación cruzada puede llevar a una evaluación sesgada del modelo.
- Conlleva al sobreajuste con los datos de entrenamiento, lo que resulta en una estimación errónea del rendimiento del modelo. Es importante realizar la sobremuestreo dentro de cadapartición de la validación cruzada para evitar este sesgo.

## Advertencias sobre Validación Cruzada

**5. Fuga de conocimientos**

- Evitar la fuga de conocimientos al preprocesar los datos antes de la validación cruzada.
- Es fundamental preprocesar los datos antes de la validación cruzada para evitar la fuga de conocimientos. Esto incluye la estandarización de características, la imputación de valores perdidos y cualquier otra transformación de datos que se realice en función de la información del conjunto de datos completo y no solo del conjunto de entrenamiento.

## Advertencias sobre Validación Cruzada

**6. No hagas Validación cruzada aleatorizadas para datos de series temporales.**

- La validación cruzada aleatorizada puede introducir sesgos en la evaluación del modelo con datos de series temporales.
- En lugar de utilizar validación cruzada aleatorizada, es preferible utilizar técnicas específicas para datos de series temporales, como la validación cruzada basada en el tiempo o la validación cruzada de bloques, que preservan la estructura temporal de los datos.

## Advertencias sobre Validación Cruzada

**7. Cuidado con la semilla a la hora de partir los datos.**

- La semilla utilizada para partir los datos puede afectar significativamente los resultados de la validación cruzada.
- La elección de la semilla puede influir en cómo se dividen los datos en conjuntos de entrenamiento y prueba, lo que a su vez puede afectar la evaluación del rendimiento del modelo. Es importante ser consistente con la semilla utilizada para garantizar la reproducibilidad de los resultados.


# 3. Pre-procesamiento

## Fases

La fase más artesanal dentro del machine learning. Ayuda a preparar los datos para ser usados con mayor eficiencia, mejorando la calidad del aprendizaje.

1. Limpieza de datos (Data Cleaning)
2. Transformación de datos (Data Transformation)
3. Codificación de datos categóricos (Categorical Data Encoding)
4. Reducción de dimensionalidad (Dimensionality Reduction)
5. Selección de características (Feature Selection)
6. Creación de características (Feature Engineering)
7. Muestreo equilibrador (Sampling)

## Limpieza de datos (Data Cleaning)

- **Descarte de variables**: Descartar variables con alta presencia de valores nulos o de variables altísimamente correlacionadas. 
- **Tratamiento de valores nulos**: Técnica de Imputación o "relleno de valores faltantes"; se puede usar métodos como imputar por algún estadístico resumen, como la media, o utilizando métodos más sofisticados como la imputación por algoritmos predictores
- **Eliminación de duplicados**: Remover filas duplicadas para evitar sesgos en el modelo.
- **Corrección de errores tipográficos**: Identificación y corrección de errores tipográficos o inconsistencias en los datos.

## Limpieza de datos (Data Cleaning) (Imputación)

- Missing Completely at Random (MCAR): Los datos se consideran MCAR cuando la probabilidad de que un dato esté perdido es completamente independiente de cualquier variable. 
  + (ej: datos perdidos por errores técnicos)

- Missing at Random (MAR): Los datos se consideran MAR cuando la probabilidad de que un dato esté perdido depende de otras variables observadas en el dataset, pero no del valor de la variable en sí misma que falta. **Los datos faltantes pueden ser predichos por otras variables observadas.**
  + (ej: las personas con educación más baja son menos propensas a declarar su ingreso))

- Missing Not at Random (MNAR): Los datos se consideran MNAR cuando la probabilidad de que un dato esté perdido depende del valor de la variable que falta. Aquí, los datos faltantes no son aleatorios y están relacionados con las propias observaciones faltantes. 
  + (ej: las personas con niveles más altos de depresión pueden ser menos propensas a responder preguntas sobre su estado mental)


En la práctica se asume que $CAR = MCAR$




# 4. Tipos de modelos

## Tipos de modelos 

- Respecto a la utilidad:
  + Explicativo 
  + Predictivo
  
- Respecto a la escalabilidad:
  + Exploratorios
  + En producción

- Respecto la necesidad:
  + Supervisado
  + No supervisado
  + Por refuerzo


## Respecto a la utilidad

|                | **Modelos explicativos** | **Modelos predictivos** |
|----------------|:-------------------------|:------------------------|
| Relación x e y | Causalidad               | Asociación              |
| Protagonista   | Teoria                   | Datos                   |
| Visión         | Retrospectiva            | Futura                  |
| Varianza       | Minimizar sesgo          | Maximizar predicción    |

## Respecto a la utilidad

|                              | Modelos explicativos                                                             | Modelos predictivos                                                                                                                  |
|-------------------|---------------------------|--------------------------|
| Objetivos                    | Establecer relaciones causales                                                   | Predecir diagnósticos actuales o resultados futuros                                                                                  |
| Precaución                   | Las posibilidades de hallazgos (Error tipo I)                                    | No ajuste: falta de generalizabilidad a nuevas poblaciones                                                                           |
| Variables candidatas         | Un conjunto limitado de factores de riesgo y factores de confusión               | Muchos predictores potenciales; algunos pueden no tener relación causal con el resultado.                                            |
| Selección de variables       | Basada en hipótesis; No debe utilizar procedimientos de selección automatizados. | Exploratorio; Puede utilizar procedimientos de selección automatizados, siempre con la correspondiente validación.                   |
| Formas de ver el rendimiento | Tamaño de los coeficientes y nivel de significación de las variables             | Discriminación (análisis ROC); calibración (Hosmer-Lemeshow); de ajuste (R2, AIC); reclasificación (índice de reclasificación neta); |
| Validación                   | Se necesitan nuevos estudios para confirmar las relaciones causales individuales | Validación interna; muestreo dividido; validación cruzada; bootstrap; validación externa.                                            |

## Respecto a la utilidad

::: columns
::: {.column width="50%"}
Los investigadores con objetivos **explicativos** se desvían.

Intentan optimizar métricas del modelo; $R^2$,curvas ROC ..., y
descuidan cuestiones como la confusión.
:::

::: {.column width="50%"}
Los investigadores con objetivos **predictivos** se desvían.

preocupándose por los coeficientes $\beta$ y valores p, y omiten omiten
pasos críticos como la calibración o la validación.
:::
:::

## Respecto a la utilidad

![](Imagenes/Dilema.jpg)

## Respecto a la utilidad

{{< video /Imagenes/Both.mp4 >}}


## Respecto a la escalabilidad:
  
  - Modelos Exploratorios: fases neofita. Utilizados a por el equipo de investigación.  No tienen alcance, ni necesidad de mantenimiento o depuración.
  
  - Modelo en producción: han sido aprobados y se requiere de que sean de uso abierto para su uso. **require de poner el modelo en la nube** y que otreos usuarios los utilicen.

## Respecto la necesidad:

1.  Aprendizaje **supervisado**
2.  Aprendizaje **no supervisado**
3.  Aprendizaje **por refuerzo**



# 4. Aprendizaje supervisado

## Aprendizaje supervisado

"El aprendizaje supervisado es la tarea de crear una función que assigna
una entrada a una salida basándose en pares de entrada-salida de
ejemplo."

Kevin P. Murphy, *Machine Learning: A Probabilistic Perspective*

## Aprendizaje supervisado

-   ::: {style="color:#8ABF74;"}
    Relativamente, el tipo de modelo más fiable y conocido
    :::

-   ::: {style="color:#BF1E1A;"}
    Requiere de completa supervisión humana
    :::

-   ::: {style="color:#BF1E1A;"}
    Los datos pueden contener error humano, ergo los algoritmos aprendan
    incorrectamente
    :::

-   Tipos de modelos:

    -   **clasificación**: Variables factor.
    -   **regresion**: Variables numericas.





## Aprendizaje supervisado

::: columns
::: {.column width="50%"}
![](Imagenes/armas%20de%20destruccion%20matematica.jpg){width="328"}
:::

::: {.column width="50%"}
*"Los privilegiados son analizados por personas, las masas por
máquinas."*

**Cathy O'Neil**
:::
:::

## Aprendizaje supervisado

-   Educación: Despido de profesores en Washington DC y Chicago por dar
    demasiado peso en la fórmula las bajas calificaciones de los
    alumnos.
-   Justicia: Janet Collins y su esposo Malcolm a la cárcel por una
    probabilidad de 1 entre 12 millones.
-   Política: microtargeting político en redes para dirigir campañas.
-   Economía: sesgos en el crédito y trabajo de tipo espiral
    retroalimentativa.

## Aprendizaje supervisado

1)  Regresión
2)  Árboles de decisión
3)  Naïve Bayes
4)  Técnicas de Supervivencia
5)  Support Vector Machines (SVM)
6)  SARIMA models
7)  Redes neuronales

##  Regresión

- Definición: Modelos que predicen una variable numérica continua.

- Forma básica (lineal): 

$Y = \beta_0 + \beta X +\epsilon$

- Forma completa:

$Y = f\left( \beta_0 + \beta X + \lambda \left( \frac{1 - \alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2} + \alpha \sum_{j=1}^{p}|\beta_{j}| \right) + \epsilon \right)$

##  Regresión

- Segun la funcion de enlace:
  + Regresión lineal.
  + Regresión múltiple.
  + Regresión polinómica.

- Según la restricción:
  + Regresión Lasso (elimina coeficientes)
  + Regresión Ridge (elimina colinealidad)
  + Regresión *elastic net* (ambos mundos, pero un poco más malos que su versión sola)
  
- Según el diseño experimental:
  + Efectos fijos
  + Efectos aleatorios
  + Efectos mixtos

##  Regresión

```{r}
# Cargar librerías
library(ggplot2)

# Dataset de ejemplo
data(mtcars)

# Modelo de regresión lineal
modelo <- lm(mpg ~ wt + hp, data = mtcars)

# Resumen del modelo
summary(modelo)

# Visualización
plot(modelo)
```
##  Regresión

Contras y Advertencias:

- Sensibilidad a valores atípicos. 
- Multicolinealidad entre variables independientes.
- Suposiciones estrictas deben ser verificadas.
- Puede no capturar relaciones no lineales, incluso las simples.


##  Árboles de decisión

Definición: Modelos que segmentan los datos en ramas según condiciones.

Características:

- Interpretabilidad fácil.
- No necesita suposiciones sobre la distribución de datos.
- Tipos: 
  * Clasificación 
  * regresión (CART).
  
##  Árboles de decisión

Conceptos Clave:

- Nodo raíz, 
- nodos internos
- hojas.

Criterios de partición: 

- Gini, 
- entropía, 
- varianza.

## Árboles de decisión

```{r}
# Cargar librerías
library(rpart)
library(rpart.plot)

# Dataset de ejemplo
data(iris)

# Modelo de árbol de decisión
modelo <- rpart(Species ~ ., data = iris, method = "class")

# Visualización del árbol
rpart.plot(modelo)
```

## Árboles de decisión

Contras y Advertencias:

- Tendencia a sobreajuste (overfitting). Sobre en algoritmos de árbol más complejos
- Muy sensibles a pequeñas variaciones en los datos. (difíciles de tener en producción bien depurados)
- Pueden generar árboles muy complejos si no se poda correctamente.

## Random Forest
Definición: Conjunto de árboles de decisión que mejora la precisión y robustez.

Características:
- Utiliza el bagging (bootstrap aggregating).
- Promedia los resultados de múltiples árboles.
- Reduce la varianza del árbol de decisión y mejora la generalización.

## Random Forest

```{r}
# Cargar librerías
library(randomForest)

# Dataset de ejemplo
data(iris)

# Modelo Random Forest
modelo <- randomForest(Species ~ ., data = iris, ntree = 100)

# Importancia de las variables
importance(modelo)

# Visualización del error
plot(modelo)

```

## Random Forest

Contras y Advertencias:
- Requiere bastantes más recursos computacionales que un solo árbol.
- Interpretación más compleja debido al conjunto de modelos. 
- No es ideal para datasets con un número muy alto de características debido al riesgo de sobreajuste.



## Naïve Bayes

Definición: Clasificador basado en la probabilidad Bayesiana.

Características:
- Supone independencia entre predictores.
- Muy eficiente en términos de tiempo.
- Funciona bien con *datos pequeños y ruidosos*.

## Naïve Bayes

```{r}
# Cargar librerías
library(e1071)

# Dataset de ejemplo
data(iris)

# Modelo Naïve Bayes
modelo <- naiveBayes(Species ~ ., data = iris)

# Predicciones
predicciones <- predict(modelo, iris)

# Matriz de confusión
table(predicciones, iris$Species)

```

## Naïve Bayes

Contras y Advertencias:

- La suposición de independencia rara vez se cumple en la práctica.
- Suele ser superado por modelos más complejos.
- No maneja bien características continuas, requiere de discretización sin discretización.

## Técnicas de Supervivencia

Definición: Modelos que analizan el tiempo hasta que ocurre un evento.

Conceptos Clave:
- Función de supervivencia: Probabilidad de que el evento no ocurra hasta cierto tiempo.
- Función de riesgo: Tasa instantánea de ocurrencia del evento en el tiempo t.

Modelos Comunes:
- **Kaplan-Meier**: Estimación no paramétrica de la función de supervivencia.
- **Modelo de Cox**: Análisis de riesgos proporcionales para estudiar el efecto de variables explicativas.
- **Modelos de Vida Acelerada**: Analiza el tiempo hasta el evento con supuestos diferentes a los riesgos proporcionales.

## Técnicas de Supervivencia / Kaplan_Meyer

```{r}
# Cargar librerías
library(survival)

# Dataset de ejemplo
data(lung)

# Modelo Kaplan-Meier
modelo <- survfit(Surv(time, status) ~ sex, data = lung)

# Visualización
plot(modelo, col = c("red", "blue"))

```

## Técnicas de Supervivencia / Kaplan_Meyer

Contras y Advertencias:

- No ajusta para variables explicativas múltiples.
- Asume que censura es independiente del riesgo de evento.
- No permite modelar efectos de covariables en la supervivencia.


##  Técnicas de Supervivencia / Cox

```{r}
# Cargar librerías
library(survival)
library(survminer)

# Dataset de ejemplo
data(lung)

# Modelo de Cox
cox_fit <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data = lung)

# Resumen del modelo
summary(cox_fit)

# Visualización del efecto de las covariables
ggcoxzph(cox.zph(cox_fit))

```


##  Técnicas de Supervivencia / Cox

Contras y Advertencias:
- Supone riesgos proporcionales, lo cual puede no ser realista.
- Requiere suficiente tamaño muestral para estimaciones confiables.
- Interpretación de coeficientes requiere cuidado, especialmente con interacciones y variables categóricas.

## Técnicas de Supervivencia / AFT

Definición: Modelos que asumen que el efecto de las covariables acelera o desacelera el tiempo hasta el evento.

Características:
- No supone riesgos proporcionales.
- Modela directamente el tiempo hasta el evento.
- Puede utilizar diversas distribuciones (Weibull, log-normal, log-logistic).

## Técnicas de Supervivencia / AFT

```{r}
# Cargar librerías
library(survival)

# Dataset de ejemplo
data(lung)

# Modelo de Vida Acelerada (Weibull)
aft_fit <- survreg(Surv(time, status) ~ age + sex + ph.ecog, data = lung, dist = "weibull")

# Resumen del modelo
summary(aft_fit)

# Predicción de tiempos medios
predict(aft_fit, type = "response")

```

## Técnicas de Supervivencia / AFT




##  Support Vector Machines

Definición: Modelos que encuentran un hiperplano óptimo para la clasificación.

Características:
- Eficaz en espacios de alta dimensión.
- Utiliza un subconjunto de puntos de entrenamiento (vectores de soporte).
- Tipos de kernel:
  + lineal
  + polinomial
  + radial.


##  Support Vector Machines

```{r}
# Cargar librerías
library(e1071)

# Dataset de ejemplo
data(iris)

# Modelo SVM
modelo <- svm(Species ~ ., data = iris, kernel = "linear")

# Predicciones
predicciones <- predict(modelo, iris)

# Matriz de confusión
table(predicciones, iris$Species)

```

##  Support Vector Machines

Contras y Advertencias:

- Muy sensible a su parametrización. 
- Suele requerir de mucho tiempo de entrenamiento con grandes datasets.
- La interpretabildiad del modelo no es muy intuitiva cuando hay más de 2 casos.


##  SARIMA

Definición: Modelos de series temporales que combinan autorregresión y media móvil.

Características:

- Captura estacionalidad y tendencias en los datos.
- Basado en diferencias para hacer la serie estacionaria.
- Parámetros: p, d, q (ARIMA); P, D, Q, s (SARIMA).

##  SARIMA

```{r}
# Cargar librerías
library(forecast)

# Dataset de ejemplo
data(AirPassengers)

# Modelo SARIMA
modelo <- auto.arima(AirPassengers)

# Predicciones
predicciones <- forecast(modelo, h = 12)

# Visualización
plot(predicciones)

```


##  SARIMA

Contras y Advertencias:

- Requiere series temporales **estacionarias**.
- La selección de parámetros puede ser compleja.
- Sensible a datos faltantes o anomalías.

##  Redes neuronales

Definición: Modelos inspirados en la estructura del cerebro humano para el aprendizaje.
Características:
- Capacidad de aprender relaciones no lineales complejas.
- Compuesto de capas (entrada, ocultas, salida).
- Utiliza Algoritmo de retropropagación para el aprendizaje.

##  Redes neuronales

```{r}
# Cargar librerías
library(neuralnet)

# Dataset de ejemplo
data(iris)

# Preprocesamiento
iris$Species <- as.numeric(iris$Species) - 1

# Modelo de red neuronal
modelo <- neuralnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris, hidden = 3)

# Visualización del modelo
plot(modelo)

```

##  Redes neuronales

Contras y Advertencias:

- Requiere gran cantidad de datos para buen rendimiento.
- el tiempo de entrenamiento puede ser largo.
- El más complicado de interpretar y explicar resultados.


Riesgo de sobreajuste sin regularización adecuada.


# 5. Aprendizaje no supervisado

## Caracteristicas principales

-   Analiza y agrupa conjuntos de datos sin etiquetar.

-   Se basan en distancias. (Euclídeas o mahalanobis)

-   ::: {style="color:#8ABF74;"}
    Solución ideal para el análisis de datos exploratorios.
    :::

-   ::: {style="color:#BF1E1A;"}
    Computacionalmente duros
    :::

-   ::: {style="color:#BF1E1A;"}
    La interpretación puede ser un infierno
    :::


## Agrupación en clústeres

-   La agrupación en clústeres de k-medias
-   Agrupación en clústeres jerárquica (HCA)
-   Agrupación en clústeres probabilística (GMM)

## Aprendizaje no supervisado: Reglas de asociación

*Identifican la probabilidad de consumir un producto dado el consumo de
otro producto*

## Aprendizaje no supervisado: Reducción de dimensionalidad

-   Análisis de componentes principales (PCA)

Este método utiliza una transformación lineal para crear una nueva
representación de datos

-   Descomposición en valores singulares

factoriza una matriz, A, en tres matrices de rango inferior.

## Aprendizaje no supervisado, caso Target

::: columns
::: {.column width="50%"}
![](Imagenes/target.png){width="359"}
:::

::: {.column width="50%"}
1.  Fundada en 1962
2.  Es la sexta empresa de venta al por menor más grande de Estados
    Unidos.
3.  351.000 empleados
:::
:::

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_embarazo.png)

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_dinero.png)

## Aprendizaje no supervisado, caso Target

```{r }
Target_ejemplo <- tibble(
  ClienteID = 1:6,
  VitaminasPrenatales = c(1, 0, 0, 0, 1, 0),
  LocionSinFragancia = c(1, 0, 0, 0, 1, 0),
  JabonSinFragancia = c(0, 1, 0, 0, 1, 0),
  Calcio = c(0, 1, 0, 0, 1, 1),
  Panales = c(0, 0, 1, 0, 0, 1),
  Toallitas = c(0, 0, 1, 0, 0, 0)
)

knitr::kable(Target_ejemplo)

```

## Aprendizaje no supervisado, caso Target

```{r echo=TRUE}
# Aplicar K-means
set.seed(123) # Para reproducibilidad
kmeans_result <- stats::kmeans(Target_ejemplo[,-1], centers = 3)

# Añadir los resultados del cluster al dataframe
Target_ejemplo <- Target_ejemplo %>%  
  mutate(Cluster = as_factor(kmeans_result$cluster) ) %>% 
  select(ClienteID, Cluster, everything())
```

## Aprendizaje no supervisado, caso Target

```{r}
knitr::kable(Target_ejemplo)
```

## Aprendizaje no supervisado, caso Target

```{r}
Target_ejemplo %>%  
  pivot_longer(
    cols= -c(Cluster,ClienteID),
    names_to =  'Producto',
    values_to = 'Compra_NoCompra') %>% 
  mutate(Compra_NoCompra = as_factor(Compra_NoCompra) ) %>% 
  ggplot(aes(x= Cluster, y= Producto, fill= Compra_NoCompra ) ) +
  geom_tile()
```

# 6. Aprendizaje por refuerzo

## Funcion

-   Entrena al software para que tome decisiones a fin de lograr los
    mejores resultados.

-   Ensayo y error (Supervisado y no supervisado al mismo tiempo)

## Aprendizaje por refuerzo

1.  Sobresale en entornos complejos

2.  Optimiza de acuerdo con objetivos a largo plazo

3.  Requiere menos interacción humana

## Aprendizaje por refuerzo (Mario)

{{< video /Imagenes/Mario_IA_1.mp4 >}}

## Aprendizaje por refuerzo (Mario)

{{< video /Imagenes/Mario_IA_2.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_1.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_2.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_3.mp4 >}}

## Aprendizaje por refuerzo (Casos famosos)

**AlphaGo y AlphaZero**: Desarrollados por DeepMind, estos sistemas lograron hitos significativos en juegos complejos como el Go, el ajedrez y el shogi. 

- *AlphaGo* derrotó al campeón mundial de Go, Lee Sedol, en 2016 
- *AlphaZero* superó a los mejores programas de ajedrez y shogi tras solo unas pocas horas de autoentrenamiento, aprendiendo desde cero sin datos previos.

## Aprendizaje por refuerzo (Casos famosos)

**StarCraft II**: AlphaStar, también de DeepMind, logró un desempeño notable en StarCraft II, otro juego de estrategia en tiempo real muy complejo. 

En 2019, **AlphaStar compitió y venció a jugadores profesionales**, demostrando habilidades avanzadas de planificación, microgestión y toma de decisiones y **sin superar el número de comandos por minuto que un jugador profesional promedio hacía**.








# 7 Métricas

## 7.1 Métricas para modelos de regresión

Toda métrica de evaluación para un modelo de machine learning
supervisado se basa en el siguiente concepto:

$$
e_{i} = original_{i} - prediccion_{i}
$$
$$
e_{i} = y_{i} - \hat{y_{i}}
$$

$$
e_{i} = f(y_{i} - \hat{y_{i}})
$$

## Métricas: MAE

-   El **Error Absoluto Medio** (MAE por sus siglas en inglés *Mean
    Absolute Error*)

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, sin considerar su dirección. $$
      MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*\hat{y}\_{i}\* es el valor predicho por el modelo.

## Métricas: MAE

-   Métrica que evalua el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia absoluta promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los
    datos originales.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   No penaliza en exceso los valores atípicos (outliers) o errores
        extremos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::

    Trata todos los errores por igual, independientemente de si son
    positivos o negativos.

## Métricas: MAE

-   Un MAE de 0 indica que el modelo predice exactamente los valores
    reales.
-   $MAE  > 0$ indica que el modelo tiene un mayor error promedio.
-   No penaliza ni discrimina entre errores grande o pequeños, es un
    promedio directo.

## Métricas: MAPE

-   El **Error Absoluto Medio Porcentual** (MAPE por sus siglas en
    inglés *Mean Absolute Percentage Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, expresada como un porcentaje. $$
      MAPE = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: MAPE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error relativo. Basada en la **diferencia porcentual
    promedio absoluta**.

-   ::: {style="color:#8ABF74;"}
    Expresa el error como un porcentaje, lo cual facilita la
    interpretación y comparación entre diferentes modelos y escalas.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser muy grande o indefinida si los valores observados son
        cercanos a cero.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::

    Trata todos los errores por igual, independientemente de si son
    positivos o negativos.

## Métricas: MAPE

-   Un MAPE de 0% indica que el modelo predice exactamente los valores
    reales.
-   MAPE \> 0% indica que el modelo tiene un mayor error porcentual
    promedio.
-   Es útil para comparar la precisión de modelos en diferentes escalas
    y unidades, pero puede ser problemático con valores observados
    cercanos a cero.

## Métricas: MSE

-   El **Error Cuadrático Medio** (MSE por sus siglas en inglés *Mean
    Squared Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, considerando la dirección y penalizando los errores
    grandes más que los pequeños. $$
      MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: MSE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    -   Penaliza los errores grandes más que los pequeños debido a la
        elevación al cuadrado de las diferencias.
    :::

-   ::: {style="color:#BF1E1A;"}
    Se utiliza principalmente en análisis teóricos y en optimización de
    modelos de alta complejidad.
    :::

    -   las unidades del error son al cuadrado, puede ser difícil de
        interpretar directamente en términos de la escala original de
        los datos.

-   ::: {style="color:#BF1E1A;"}
    **Es sensible a los valores atípicos**.
    :::

    Unos pocos valores atípicos pueden incrementar significativamente el
    MSE.

## Métricas: MSE

-   Un MSE de 0 indica que el modelo predice exactamente los valores
    reales.
-   MSE \> 0 indica que el modelo tiene un mayor error promedio.
-   Penaliza más los errores grandes que los pequeños. Útil en contextos
    donde los errores grandes son especialmente perjudiciales.

## Métricas: RMSE

-   El **Error Cuadrático Medio Raiz** (RMSE por sus siglas en inglés
    *Root Mean Squared Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, considerando su dirección. $$
      RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: RMSE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los
    datos originales.
    :::

-   Penaliza altamente los valores atípicos (outliers) o errores
    extremos debido a la elevación al cuadrado de las diferencias y
    luego devolverlos a la escala original.

-   ::: {style="color:#BF1E1A;"}
    Es **muy** sensible a los valores atípicos.
    :::

## Métricas: RMSE

-   Un RMSE de 0 indica que el modelo predice exactamente los valores
    reales.
-   RMSE \> 0 indica que el modelo tiene un mayor error promedio.
-   Penaliza los errores grandes más que los pequeños, por lo tanto, es
    útil en contextos donde los errores grandes son especialmente
    perjudiciales.

## Métricas: $R^2$

-   El **Coeficiente de Determinación** $R^2$.

-   Medida que indica la proporción de la variabilidad de la variable
    dependiente que es explicada por el modelo. $$
      R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   $\hat{y}_{i}$ es el valor predicho por el modelo.
    -   $\bar{y}$ es la media de los valores observados.

## Métricas: $R^2$

-   Métrica que evalúa el rendimiento del modelo en términos de la
    proporción de variabilidad explicada. Basada en la **relación entre
    la suma de los errores residuales y la suma total de los
    cuadrados**.

-   ::: {style="color:#8ABF74;"}
    Facilita la interpretación de la efectividad del modelo, ya que
    varía entre 0 y 1.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser engañoso en modelos no lineales o con muchos
        parámetros(atención modelos predictivos)
    :::

-   ::: {style="color:#BF1E1A;"}
    **No informa sobre el sesgo o la varianza del modelo**.
    :::

    Solo indica la proporción de la varianza explicada.

## Métricas: $R^2$

-   Un $R^2$ de 1 indica que el modelo explica toda la variabilidad de
    los datos.
-   $R^2 = 0$ indica que el modelo no explica ninguna variabilidad de
    los datos.
-   Valores negativos de $R^2$ pueden ocurrir y indican que el modelo es
    peor que uno que simplemente predice la media de los datos.

## Métricas: Huber Loss

-   también conocida como **error cuadrático medio suavizado**

-   Ccombina el MAE con MSE, siendo menos sensible a los valores
    atípicos que el MSE y más robusta que el MAE. $$
      L_\delta(y, \hat{y}) = 
      \begin{cases} 
      \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \le \delta \\
      \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{si } |y - \hat{y}| > \delta
      \end{cases}
      $$ Donde:

    -   *y* es el valor observado.
    -   $\hat{y}$ es el valor predicho por el modelo.
    -   $\delta$ es un umbral positivo que determina la transición entre
        el MAE y el MSE.

## Métricas: Huber Loss

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error, combinando **propiedades del MAE y el MSE**.

-   La elección de $\delta$ es crucial y puede influir en la
    sensibilidad del modelo a los valores atípicos.

-   Muy utlizado en técnicas de optimización basadas en gradientes.
    (XGBoost)

## Métricas: Huber Loss

-   Un valor de Huber Loss cercano a 0 indica que el modelo predice con
    alta precisión.
-   La Huber Loss se ajusta dependiendo de la elección de $\delta$, lo
    que permite controlar el equilibrio entre penalización de errores
    grandes y pequeños.
-   Resumen información del MAE i el MSE, de modo que vale la pena
    computar ambos para tener un contexto amplio de la situación.

## Métricas: ICC

-   El **Índice de Ideabilidad de Correlación** (ICC por sus siglas en
    inglés *Index of Ideality of Correlation*).

-   Medida que evalúa la precisión predictiva de un modelo al comparar
    los valores predichos con los valores observados. $$
      ICC = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \overline{y})^2 + \sum_{i=1}^{n} (\hat{y}_i - \overline{\hat{y}})^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.
    -   *(*\overline{y}) es la media de los valores observados.
    -   *(*\overline{\hat{y}}) es la media de los valores predichos.

## Métricas: ICC

-   Métrica que evalúa el rendimiento del modelo en términos de
    precisión predictiva. Basada en la **relación entre la suma de los
    errores residuales y las sumas de las desviaciones totales y
    predichas**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida comprensible de la precisión predictiva del
    modelo.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser sensible a la distribución de los valores observados y
        predichos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede ser influenciado por la escala de los datos**.
    :::

    Los resultados pueden variar dependiendo de la escala de los valores
    observados y predichos.

## Métricas: ICC

-   Un ICC de 1 indica que el modelo predice exactamente los valores
    reales.
-   ICC cercano a 0 indica que el modelo no tiene una buena precisión
    predictiva.
-   ICC puede ser negativo si el modelo tiene un rendimiento peor que el
    promedio.

## Métricas: AIC

-   El Criterio de Información de Akaike (Akaike 1974), AIC por sus
    siglas en inglés *Akaike Information Criterion*.

    $$
    AIC = 2{k} - \ln(\mathcal{L}) 
    $$

    Donde:

    -   *k* es el numero de parametros
    -   $\mathcal{L}$ es la verosimilitud del modelo.

## Métricas: AIC

-   Métrica basada en **teoría de información**, la cual balancea los
    poderes explicativos y predictivos del modelo

-   ::: {style="color:#8ABF74;"}
    Castiga el uso de muchas variables.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Un AIC por si solo no tiene ningún valor**.
    :::

    Se necesitan de otras métricas o compararse con el AIC de otro
    modelo2 para darle contexto.

## Métricas: AIC

![](Imagenes/AIC%20vs%20R2.png){width="711" height="469"}

## Casos de interpretación 1

- Supongamos que hemos construido un modelo de tipo regresión para 
- **Objetivo**: redecir los ingresos anuales de individuos basándonos en su nivel educativo (en años de educación formal). 
- Nuestro pre-estudio de la población sugiere que el promedio de ingresos está alrededor de 25.000€. 

Se evalua el modelo con el conjunto de entrenamiento. Las métricas son las siguientes:

- MAE (Mean Absolute Error): 2000
- RMSE (Root Mean Squared Error): 2500 
- $R^2$ : 0.85. 

## Casos de interpretación 1

Evaluación del Rendimiento Global:

- $RMSE >\sim MAE$ los errores grandes no son significativamente predominantes. La pequeña diferencia entre RMSE y MAE sugiere que no hay muchos outliers importantes afectando el modelo. El modelo está capturando de manera **robusta** la relación principal sin verse afectado por valores extremos.

- El 85% de la variabilidad en los ingresos puede ser explicada por el nivel educativo. $R^2$


## Casos de interpretación 2

- Supongamos que hemos construido un modelo de tipo regresión para 
- **Objetivo**: redecir los ingresos anuales de individuos basándonos en su nivel educativo (en años de educación formal). 
- Nuestro pre-estudio de la población sugiere que el promedio de ingresos está alrededor de 25.000€. 

Se evalua el modelo con el conjunto de entrenamiento. Las métricas son las siguientes:

- MAE (Mean Absolute Error): 20000
- RMSE (Root Mean Squared Error): 45000 

## Casos de interpretación 2

Evaluación del Rendimiento Global:

- La escala de los errores de demasiado grande respecto al promedio poblacional. No son resultados admisibles. 

- $RMSE >> MAE$ los errores grandes son significativamente predominantes. La gran diferencia entre RMSE y MAE sugiere que hay muchos outliers importantes afectando el modelo. 

- Es muy posible que haya outliers tan fuera de escala que hayan desasjustado el modelo. 




## 7.2 Métricas para modelos clasifiación

Toda métrica de evaluación para un modelo de machine learning
supervisado se basa en el siguiente concepto:

|                         | Prediccion TRUE     | Prediccion FALSE    |
|-------------------------|---------------------|---------------------|
| **Dato original TRUE**  | TRUE Positivo (TP)  | FALSE Positive (FP) |
| **Dato original FALSE** | Falso Negativo (FN) | True Negativo (TN)  |



## Métricas: Accuracy

- La **Precisión** (Accuracy).

- Medida que evalúa la proporción de predicciones correctas sobre el total de predicciones realizadas.
    $$
    Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *TN* es el número de verdaderos negativos.
    -   *FP* es el número de falsos positivos.
    -   *FN* es el número de falsos negativos.
    
## Métricas: Accuracy

- Métrica que evalúa el rendimiento del modelo en términos de **predicciones correctas**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar y calcular.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser engañosa en conjuntos de datos desbalanceados.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No distingue entre tipos de errores**.
    :::
    Trata todos los errores por igual, independientemente de si son falsos positivos o falsos negativos.

## Métricas: Accuracy

- Una Accuracy de 1 indica que el modelo predice todas las clases correctamente.
- Accuracy cercana a 0 indica un bajo rendimiento del modelo.
- En contextos con clases desbalanceadas, otras métricas suelen ser más informativas.




## Métricas: Precision

- La **Precisión** (Precision), también conocida como **Valor Predictivo Positivo**.

- Medida que evalúa la proporción de verdaderos positivos sobre el total de predicciones positivas realizadas por el modelo.
    $$
    Precision = \frac{TP}{TP + FP}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *FP* es el número de falsos positivos.
    
## Métricas: Precision

- Métrica que evalúa el rendimiento del modelo en términos de **exactitud de las predicciones positivas**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial minimizar los falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de reducir la sensibilidad.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera los falsos negativos**.
    :::
    Puede no ser suficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Precision

- Una Precisión de 1 indica que todas las predicciones positivas del modelo son correctas.
- Precisión cercana a 0 indica un alto número de falsos positivos.
- Es útil en contextos donde los falsos positivos son más costosos que los falsos negativos.





## Métricas: Sensibilidad

- La **Sensibilidad** (también conocida como **Recall** o **Tasa de Verdaderos Positivos**).

- Medida que evalúa la proporción de verdaderos positivos sobre el total de positivos.
    $$
    Sensibilidad = \frac{TP}{TP + FN}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *FN* es el número de falsos negativos.
    



    
## Métricas: Sensibilidad

- Métrica que evalúa el rendimiento del modelo en términos de **capacidad para identificar correctamente los positivos**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial identificar correctamente todos los casos positivos. (enfermedad)
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de aumentar los falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la tasa de falsos positivos**.
    :::
    Puede no ser suficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Sensibilidad

- Una Sensibilidad de 1 indica que el modelo identifica correctamente todos los positivos reales.
- Sensibilidad cercana a 0 indica que el modelo identifica incorrectamente la mayoría de los positivos reales.
- Es útil en contextos donde los falsos negativos son más costosos que los falsos positivos.



## Métricas: Especificidad

- La **Especificidad** (también conocida como **Tasa de Verdaderos Negativos**).

- Medida que evalúa la proporción de verdaderos negativos sobre el total de negativos.
    $$
    Especificidad = \frac{TN}{TN + FP}
    $$
    Donde:
    -   *TN* es el número de verdaderos negativos.
    -   *FP* es el número de falsos positivos.
    
## Métricas: Especificidad

- Métrica que evalúa el rendimiento del modelo en términos de **capacidad para identificar correctamente los negativos**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial identificar correctamente todos los casos negativos. (Inclusión)
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de aumentar los falsos negativos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la tasa de falsos negativos**.
    :::
    Insuficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Especificidad

- Una Especificidad de 1 indica que el modelo identifica correctamente todos los negativos reales.
- Especificidad cercana a 0 indica que el modelo identifica incorrectamente la mayoría de los negativos reales.
- Es útil en contextos donde los falsos positivos son más costosos que los falsos negativos.



## Métricas: J-Index

- El **Índice de Youden** (J-Index por sus siglas en inglés).

- Medida que combina la sensibilidad y la especificidad para evaluar el rendimiento general del modelo.
    $$
    J = Sensibilidad + Especificidad - 1
    $$
    Donde:
    -   *Sensibilidad* es la tasa de verdaderos positivos.
    -   *Especificidad* es la tasa de verdaderos negativos.
    
## Métricas: J-Index

- Métrica que evalúa el rendimiento del modelo en términos de **balance entre sensibilidad y especificidad**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida equilibrada del rendimiento del modelo considerando tanto los verdaderos positivos como los verdaderos negativos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - No pondera la importancia relativa de sensibilidad y especificidad, no distingue entre tipos de errores.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede no ser suficiente por sí solo**.
    :::
    Es útil en combinación con otras métricas para una evaluación más completa.

## Métricas: J-Index

- Un J-Index de 1 indica que el modelo predice perfectamente tanto los positivos como los negativos reales.
- J-Index cercano a 0 indica un bajo rendimiento del modelo.
- Es útil para comparar modelos en términos de su capacidad global para diferenciar entre clases.

## Métricas: F1 Score

- El **F1 Score**.

- Medida que combina la precisión y la sensibilidad en una única métrica armonizada.
    $$
    F1\ Score = 2 \times \frac{Precision \times Sensibilidad}{Precision + Sensibilidad}
    $$
    Donde:
    -   *Precision* es la precisión.
    -   *Sensibilidad* es la sensibilidad (también conocida como recall).
    
## Métricas: F1 Score

- Métrica que evalúa el rendimiento del modelo en términos de **balance entre precisión y sensibilidad**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida equilibrada del rendimiento del modelo cuando hay una distribución desigual de clases.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede no ser suficiente por sí sola en todos los contextos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede ser influenciada por clases desbalanceadas**.
    :::
    Es útil en combinación con otras métricas para una evaluación más completa.

## Métricas: F1 Score

- Un F1 Score de 1 indica que el modelo tiene perfecta precisión y sensibilidad.
- F1 Score cercano a 0 indica un bajo rendimiento del modelo.
- Es útil cuando se busca un equilibrio entre precisión y sensibilidad, especialmente en conjuntos de datos desbalanceados.




## Métricas: False Discovery Rate

- La **Tasa de Falsos Descubrimientos** (False Discovery Rate, FDR).

- Medida que evalúa la proporción de falsos positivos sobre el total de predicciones positivas realizadas por el modelo.
    $$
    FDR = \frac{FP}{TP + FP}
    $$
    Donde:
    -   *FP* es el número de falsos positivos.
    -   *TP* es el número de verdaderos positivos.
    
## Métricas: False Discovery Rate

- Métrica que evalúa el rendimiento del modelo en términos de **proporción de falsos positivos** en las predicciones positivas.

-   ::: {style="color:#8ABF74;"}
    Útil en contextos donde es crucial controlar el número de falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta si el modelo tiene baja precisión.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera los verdaderos negativos ni los falsos negativos**.
    :::
    Se enfoca solo en las predicciones positivas.

## Métricas: False Discovery Rate

- Una FDR de 0 indica que no hay falsos positivos en las predicciones positivas del modelo.
- FDR cercana a 1 indica que la mayoría de las predicciones positivas son incorrectas.
- Es útil en contextos donde los falsos positivos son más costosos y deben ser minimizados.




## Casos de interpretación 4

**GIRO DRAMÁTICO DE LOS ACONTECIMIENTOS**

Las métricas son las siguientes:

- Accuracy (Exactitud): 0.5 [precisión general de una moneda al aire]
- Sensitivity (Sensibilidad o Recall): 0.1 [acierta 1 de cada 10 a los estudiantes en riesgo de abandono escolar ]
- Specificity (Especificidad): 0.90 [9 de 10 identificará estudiantes que no tienen riesgo de desertar. Mr. evidencias]
- False Discovery Rate (FDR): 0.8 [4 de cada 5 intervenciones podrían ser innecesarias, administración ineficiente]








