---
title: "Que comience el periplo de la Modelización"
format:
  revealjs:
    incremental: true  
    scrollable: true
    transition: slide
editor: 
  markdown: 
    wrap: 72
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse)
```

# 1. Bases de modelización

## Bases de modelización

1.  Tener clara la pregunta
2.  Tener más o menos claras las variables que interfieren en el proceso
3.  Tener Una idea de la complejidad de la pregunta

# 2. Tipos de modelos

## Tipos de modelos 

|                | **Modelos explicativos** | **Modelos predictivos** |
|----------------|:-------------------------|:------------------------|
| Relación x e y | Causalidad               | Asociación              |
| Protagonista   | Teoria                   | Datos                   |
| Visión         | Retrospectiva            | Futura                  |
| Varianza       | Minimizar sesgo          | Maximizar predicción    |

## Tipos de modelos

|                              | Modelos explicativos                                                             | Modelos predictivos                                                                                                                  |
|-------------------|---------------------------|--------------------------|
| Objetivos                    | Establecer relaciones causales                                                   | Predecir diagnósticos actuales o resultados futuros                                                                                  |
| Precaución                   | Las posibilidades de hallazgos (Error tipo I)                                    | No ajuste: falta de generalizabilidad a nuevas poblaciones                                                                           |
| Variables candidatas         | Un conjunto limitado de factores de riesgo y factores de confusión               | Muchos predictores potenciales; algunos pueden no tener relación causal con el resultado.                                            |
| Selección de variables       | Basada en hipótesis; No debe utilizar procedimientos de selección automatizados. | Exploratorio; Puede utilizar procedimientos de selección automatizados, siempre con la correspondiente validación.                   |
| Formas de ver el rendimiento | Tamaño de los coeficientes y nivel de significación de las variables             | Discriminación (análisis ROC); calibración (Hosmer-Lemeshow); de ajuste (R2, AIC); reclasificación (índice de reclasificación neta); |
| Validación                   | Se necesitan nuevos estudios para confirmar las relaciones causales individuales | Validación interna; muestreo dividido; validación cruzada; bootstrap; validación externa.                                            |

## Tipos de modelos 

::: columns
::: {.column width="50%"}
Los investigadores con objetivos **explicativos** se desvían.

Intentan optimizar métricas del modelo; $R^2$,curvas ROC ..., y
descuidan cuestiones como la confusión.
:::

::: {.column width="50%"}
Los investigadores con objetivos **predictivos** se desvían.

preocupándose por los coeficientes $\beta$ y valores p, y omiten omiten
pasos críticos como la calibración o la validación.
:::
:::

## Tipos de modelos 

![](Imagenes/Dilema.jpg)


## Tipos de modelos

1.  Aprendizaje **supervisado**
2.  Aprendizaje **no supervisado**
3.  Aprendizaje **por refuerzo**

# 3. Aprendizaje supervisado

## Aprendizaje supervisado

"El aprendizaje supervisado es la tarea de crear una función que assigna
una entrada a una salida basándose en pares de entrada-salida de
ejemplo."

Kevin P. Murphy, *Machine Learning: A Probabilistic Perspective*

## Aprendizaje supervisado

-   ::: {style="color:#8ABF74;"}
    Relativamente, el tipo de modelo más fiable y conocido
    :::

-   ::: {style="color:#BF1E1A;"}
    Requiere de completa supervisión humana
    :::

-   ::: {style="color:#BF1E1A;"}
    Los datos pueden contener error humano, ergo los algoritmos aprendan
    incorrectamente
    :::

-   Tipos de modelos:

    -   **clasificación**: Variables factor.
    -   **regresion**: Variables numericas.

## Aprendizaje supervisado

::: columns
::: {.column width="50%"}
![](Imagenes/armas%20de%20destruccion%20matematica.jpg){width="328"}
:::

::: {.column width="50%"}
*"Los privilegiados son analizados por personas, las masas por
máquinas."*

**Cathy O'Neil**
:::
:::

## Aprendizaje supervisado

-   Educación: Despido de profesores en Washington DC y Chicago por dar
    demasiado peso en la fórmula las bajas calificaciones de los
    alumnos.
-   Justicia: Janet Collins y su esposo Malcolm a la cárcel por una
    probabilidad de 1 entre 12 millones.
-   Política: microtargeting político en redes para dirigir campañas.
-   Economía: sesgos en el crédito y trabajo de tipo espiral
    retroalimentativa.

## Aprendizaje supervisado

1)  Regresión
2)  Árboles de decisión
3)  Naïve Bayes
4)  Técnicas de Supervivencia
5)  Support Vector Machines (SVM)
6)  SARIMA models
7)  Redes neuronales

## Aprendizaje supervisado (Regresión)


## Aprendizaje supervisado (Árboles de decisión)

## Aprendizaje supervisado (Naïve Bayes)

## Aprendizaje supervisado (Técnicas de Supervivencia)

## Aprendizaje supervisado (Técnicas de Supervivencia)

## Aprendizaje supervisado (Support Vector Machines)

## Aprendizaje supervisado (SARIMA models)

## Aprendizaje supervisado (Redes neuronales)




# 4. Aprendizaje no supervisado

## Aprendizaje no supervisado

-   Analiza y agrupa conjuntos de datos sin etiquetar.

-   Se basan en distancias. (Euclídeas o mahalanobis)

-   ::: {style="color:#8ABF74;"}
    Solución ideal para el análisis de datos exploratorios.
    :::

-   ::: {style="color:#BF1E1A;"}
    Computacionalmente duros
    :::

-   ::: {style="color:#BF1E1A;"}
    La interpretación puede ser un infierno
    :::

## Aprendizaje no supervisado

## Aprendizaje no supervisado: Agrupación en clústeres

-   La agrupación en clústeres de k-medias
-   Agrupación en clústeres jerárquica (HCA)
-   Agrupación en clústeres probabilística (GMM)

## Aprendizaje no supervisado: Reglas de asociación

*Identifican la probabilidad de consumir un producto dado el consumo de
otro producto*

## Aprendizaje no supervisado: Reducción de dimensionalidad

-   Análisis de componentes principales (PCA)

Este método utiliza una transformación lineal para crear una nueva
representación de datos

-   Descomposición en valores singulares

factoriza una matriz, A, en tres matrices de rango inferior.

## Aprendizaje no supervisado, caso Target

::: columns
::: {.column width="50%"}
![](Imagenes/target.png){width="359"}
:::

::: {.column width="50%"}
1.  Fundada en 1962
2.  Es la sexta empresa de venta al por menor más grande de Estados
    Unidos.
3.  351.000 empleados
:::
:::

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_embarazo.png)

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_dinero.png)

## Aprendizaje no supervisado, caso Target

```{r }
Target_ejemplo <- tibble(
  ClienteID = 1:6,
  VitaminasPrenatales = c(1, 0, 0, 0, 1, 0),
  LocionSinFragancia = c(1, 0, 0, 0, 1, 0),
  JabonSinFragancia = c(0, 1, 0, 0, 1, 0),
  Calcio = c(0, 1, 0, 0, 1, 1),
  Panales = c(0, 0, 1, 0, 0, 1),
  Toallitas = c(0, 0, 1, 0, 0, 0)
)

knitr::kable(Target_ejemplo)

```

## Aprendizaje no supervisado, caso Target

```{r echo=TRUE}
# Aplicar K-means
set.seed(123) # Para reproducibilidad
kmeans_result <- stats::kmeans(Target_ejemplo[,-1], centers = 3)

# Añadir los resultados del cluster al dataframe
Target_ejemplo <- Target_ejemplo %>%  
  mutate(Cluster = as_factor(kmeans_result$cluster) ) %>% 
  select(ClienteID, Cluster, everything())
```

## Aprendizaje no supervisado, caso Target

```{r}
knitr::kable(Target_ejemplo)
```

## Aprendizaje no supervisado, caso Target

```{r}
Target_ejemplo %>%  
  pivot_longer(
    cols= -c(Cluster,ClienteID),
    names_to =  'Producto',
    values_to = 'Compra_NoCompra') %>% 
  mutate(Compra_NoCompra = as_factor(Compra_NoCompra) ) %>% 
  ggplot(aes(x= Cluster, y= Producto, fill= Compra_NoCompra ) ) +
  geom_tile()
```

# 5. Aprendizaje por refuerzo

## 5 Aprendizaje por refuerzo

-   Entrena al software para que tome decisiones a fin de lograr los
    mejores resultados.

-   Ensayo y error (Supervisado y no supervisado al mismo tiempo)

## Aprendizaje por refuerzo

1.  Sobresale en entornos complejos

2.  Optimiza de acuerdo con objetivos a largo plazo

3.  Requiere menos interacción humana

## Aprendizaje por refuerzo (Mario)

{{< video /Imagenes/Mario_IA_1.mp4 >}}

## Aprendizaje por refuerzo (Mario)

{{< video /Imagenes/Mario_IA_2.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_1.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_2.mp4 >}}

## Aprendizaje por refuerzo (Pokemon)

{{< video /Imagenes/Pokemon_IA_3.mp4 >}}

## Aprendizaje por refuerzo (Casos famosos)

**AlphaGo y AlphaZero**: Desarrollados por DeepMind, estos sistemas lograron hitos significativos en juegos complejos como el Go, el ajedrez y el shogi. 

- *AlphaGo* derrotó al campeón mundial de Go, Lee Sedol, en 2016 
- *AlphaZero* superó a los mejores programas de ajedrez y shogi tras solo unas pocas horas de autoentrenamiento, aprendiendo desde cero sin datos previos.

## Aprendizaje por refuerzo (Casos famosos)

**StarCraft II**: AlphaStar, también de DeepMind, logró un desempeño notable en StarCraft II, otro juego de estrategia en tiempo real muy complejo. 

En 2019, **AlphaStar compitió y venció a jugadores profesionales**, demostrando habilidades avanzadas de planificación, microgestión y toma de decisiones y **sin superar el número de comandos por minuto que un jugador profesional promedio hacía**.








# 6 Métricas

## 6.1 Métricas para modelos de regresión

Toda métrica de evaluación para un modelo de machine learning
supervisado se basa en el siguiente concepto:

$$
e_{i} = original_{i} - prediccion_{i}
$$ $$
e_{i} = y_{i} - \hat{y_{i}}
$$

$$
e_{i} = f(y_{i} - \hat{y_{i}})
$$


## Métricas: MAE

-   El **Error Absoluto Medio** (MAE por sus siglas en inglés *Mean
    Absolute Error*)

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, sin considerar su dirección. $$
      MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*\hat{y}\_{i}\* es el valor predicho por el modelo.

## Métricas: MAE

-   Métrica que evalua el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia absoluta promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los
    datos originales.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   No penaliza en exceso los valores atípicos (outliers) o errores
        extremos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::

    Trata todos los errores por igual, independientemente de si son
    positivos o negativos.

## Métricas: MAE

-   Un MAE de 0 indica que el modelo predice exactamente los valores
    reales.
-   $MAE  > 0$ indica que el modelo tiene un mayor error promedio.
-   No penaliza ni discrimina entre errores grande o pequeños, es un
    promedio directo.

## Métricas: MAPE

-   El **Error Absoluto Medio Porcentual** (MAPE por sus siglas en
    inglés *Mean Absolute Percentage Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, expresada como un porcentaje. $$
      MAPE = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: MAPE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error relativo. Basada en la **diferencia porcentual
    promedio absoluta**.

-   ::: {style="color:#8ABF74;"}
    Expresa el error como un porcentaje, lo cual facilita la
    interpretación y comparación entre diferentes modelos y escalas.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser muy grande o indefinida si los valores observados son
        cercanos a cero.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::

    Trata todos los errores por igual, independientemente de si son
    positivos o negativos.

## Métricas: MAPE

-   Un MAPE de 0% indica que el modelo predice exactamente los valores
    reales.
-   MAPE \> 0% indica que el modelo tiene un mayor error porcentual
    promedio.
-   Es útil para comparar la precisión de modelos en diferentes escalas
    y unidades, pero puede ser problemático con valores observados
    cercanos a cero.

## Métricas: MSE

-   El **Error Cuadrático Medio** (MSE por sus siglas en inglés *Mean
    Squared Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, considerando la dirección y penalizando los errores
    grandes más que los pequeños. $$
      MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: MSE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    -   Penaliza los errores grandes más que los pequeños debido a la
        elevación al cuadrado de las diferencias.
    :::

-   ::: {style="color:#BF1E1A;"}
    Se utiliza principalmente en análisis teóricos y en optimización de
    modelos de alta complejidad.
    :::

    -   las unidades del error son al cuadrado, puede ser difícil de
        interpretar directamente en términos de la escala original de
        los datos.

-   ::: {style="color:#BF1E1A;"}
    **Es sensible a los valores atípicos**.
    :::

    Unos pocos valores atípicos pueden incrementar significativamente el
    MSE.

## Métricas: MSE

-   Un MSE de 0 indica que el modelo predice exactamente los valores
    reales.
-   MSE \> 0 indica que el modelo tiene un mayor error promedio.
-   Penaliza más los errores grandes que los pequeños. Útil en contextos
    donde los errores grandes son especialmente perjudiciales.

## Métricas: RMSE

-   El **Error Cuadrático Medio Raiz** (RMSE por sus siglas en inglés
    *Root Mean Squared Error*).

-   Medida de la magnitud promedio de los errores en un conjunto de
    predicciones, considerando su dirección. $$
      RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.

## Métricas: RMSE

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los
    datos originales.
    :::

-   Penaliza altamente los valores atípicos (outliers) o errores
    extremos debido a la elevación al cuadrado de las diferencias y
    luego devolverlos a la escala original.

-   ::: {style="color:#BF1E1A;"}
    Es **muy** sensible a los valores atípicos.
    :::

## Métricas: RMSE

-   Un RMSE de 0 indica que el modelo predice exactamente los valores
    reales.
-   RMSE \> 0 indica que el modelo tiene un mayor error promedio.
-   Penaliza los errores grandes más que los pequeños, por lo tanto, es
    útil en contextos donde los errores grandes son especialmente
    perjudiciales.

## Métricas: $R^2$

-   El **Coeficiente de Determinación** $R^2$.

-   Medida que indica la proporción de la variabilidad de la variable
    dependiente que es explicada por el modelo. $$
      R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   $\hat{y}_{i}$ es el valor predicho por el modelo.
    -   $\bar{y}$ es la media de los valores observados.

## Métricas: $R^2$

-   Métrica que evalúa el rendimiento del modelo en términos de la
    proporción de variabilidad explicada. Basada en la **relación entre
    la suma de los errores residuales y la suma total de los
    cuadrados**.

-   ::: {style="color:#8ABF74;"}
    Facilita la interpretación de la efectividad del modelo, ya que
    varía entre 0 y 1.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser engañoso en modelos no lineales o con muchos
        parámetros(atención modelos predictivos)
    :::

-   ::: {style="color:#BF1E1A;"}
    **No informa sobre el sesgo o la varianza del modelo**.
    :::

    Solo indica la proporción de la varianza explicada.

## Métricas: $R^2$

-   Un $R^2$ de 1 indica que el modelo explica toda la variabilidad de
    los datos.
-   $R^2 = 0$ indica que el modelo no explica ninguna variabilidad de
    los datos.
-   Valores negativos de $R^2$ pueden ocurrir y indican que el modelo es
    peor que uno que simplemente predice la media de los datos.

## Métricas: Huber Loss

-   también conocida como **error cuadrático medio suavizado**

-   Ccombina el MAE con MSE, siendo menos sensible a los valores
    atípicos que el MSE y más robusta que el MAE. $$
      L_\delta(y, \hat{y}) = 
      \begin{cases} 
      \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \le \delta \\
      \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{si } |y - \hat{y}| > \delta
      \end{cases}
      $$ Donde:

    -   *y* es el valor observado.
    -   $\hat{y}$ es el valor predicho por el modelo.
    -   $\delta$ es un umbral positivo que determina la transición entre
        el MAE y el MSE.

## Métricas: Huber Loss

-   Métrica que evalúa el rendimiento del modelo en términos de la
    magnitud del error, combinando **propiedades del MAE y el MSE**.

-   La elección de $\delta$ es crucial y puede influir en la
    sensibilidad del modelo a los valores atípicos.

-   Muy utlizado en técnicas de optimización basadas en gradientes.
    (XGBoost)

## Métricas: Huber Loss

-   Un valor de Huber Loss cercano a 0 indica que el modelo predice con
    alta precisión.
-   La Huber Loss se ajusta dependiendo de la elección de $\delta$, lo
    que permite controlar el equilibrio entre penalización de errores
    grandes y pequeños.
-   Resumen información del MAE i el MSE, de modo que vale la pena
    computar ambos para tener un contexto amplio de la situación.

## Métricas: ICC

-   El **Índice de Ideabilidad de Correlación** (ICC por sus siglas en
    inglés *Index of Ideality of Correlation*).

-   Medida que evalúa la precisión predictiva de un modelo al comparar
    los valores predichos con los valores observados. $$
      ICC = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \overline{y})^2 + \sum_{i=1}^{n} (\hat{y}_i - \overline{\hat{y}})^2}
      $$ Donde:

    -   *n* es el número de observaciones.
    -   *y*<sub>i</sub> es el valor observado.
    -   \*(\hat{y}\_i)\* es el valor predicho por el modelo.
    -   *(*\overline{y}) es la media de los valores observados.
    -   *(*\overline{\hat{y}}) es la media de los valores predichos.

## Métricas: ICC

-   Métrica que evalúa el rendimiento del modelo en términos de
    precisión predictiva. Basada en la **relación entre la suma de los
    errores residuales y las sumas de las desviaciones totales y
    predichas**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida comprensible de la precisión predictiva del
    modelo.
    :::

-   ::: {style="color:#BF1E1A;"}
    -   Puede ser sensible a la distribución de los valores observados y
        predichos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede ser influenciado por la escala de los datos**.
    :::

    Los resultados pueden variar dependiendo de la escala de los valores
    observados y predichos.

## Métricas: ICC

-   Un ICC de 1 indica que el modelo predice exactamente los valores
    reales.
-   ICC cercano a 0 indica que el modelo no tiene una buena precisión
    predictiva.
-   ICC puede ser negativo si el modelo tiene un rendimiento peor que el
    promedio.

## Métricas: AIC

-   El Criterio de Información de Akaike (Akaike 1974), AIC por sus
    siglas en inglés *Akaike Information Criterion*.

    $$
    AIC = 2{k} - \ln(\mathcal{L}) 
    $$

    Donde:

    -   *k* es el numero de parametros
    -   $\mathcal{L}$ es la verosimilitud del modelo.

## Métricas: AIC

-   Métrica basada en **teoría de información**, la cual balancea los
    poderes explicativos y predictivos del modelo

-   ::: {style="color:#8ABF74;"}
    Castiga el uso de muchas variables.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Un AIC por si solo no tiene ningún valor**.
    :::

    Se necesitan de otras métricas o compararse con el AIC de otro
    modelo2 para darle contexto.

## Métricas: AIC

![](Imagenes/AIC%20vs%20R2.png){width="711" height="469"}





## 6.2 Métricas para modelos clasifiación

Toda métrica de evaluación para un modelo de machine learning
supervisado se basa en el siguiente concepto:

|                         | Prediccion TRUE     | Prediccion FALSE    |
|-------------------------|---------------------|---------------------|
| **Dato original TRUE**  | TRUE Positivo (TP)  | FALSE Positive (FP) |
| **Dato original FALSE** | Falso Negativo (FN) | True Negativo (TN)  |





## Métricas: Accuracy

- La **Precisión** (Accuracy).

- Medida que evalúa la proporción de predicciones correctas sobre el total de predicciones realizadas.
    $$
    Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *TN* es el número de verdaderos negativos.
    -   *FP* es el número de falsos positivos.
    -   *FN* es el número de falsos negativos.
    
## Métricas: Accuracy

- Métrica que evalúa el rendimiento del modelo en términos de **predicciones correctas**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar y calcular.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser engañosa en conjuntos de datos desbalanceados.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No distingue entre tipos de errores**.
    :::
    Trata todos los errores por igual, independientemente de si son falsos positivos o falsos negativos.

## Métricas: Accuracy

- Una Accuracy de 1 indica que el modelo predice todas las clases correctamente.
- Accuracy cercana a 0 indica un bajo rendimiento del modelo.
- En contextos con clases desbalanceadas, otras métricas suelen ser más informativas.




## Métricas: Precision

- La **Precisión** (Precision), también conocida como **Valor Predictivo Positivo**.

- Medida que evalúa la proporción de verdaderos positivos sobre el total de predicciones positivas realizadas por el modelo.
    $$
    Precision = \frac{TP}{TP + FP}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *FP* es el número de falsos positivos.
    
## Métricas: Precision

- Métrica que evalúa el rendimiento del modelo en términos de **exactitud de las predicciones positivas**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial minimizar los falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de reducir la sensibilidad.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera los falsos negativos**.
    :::
    Puede no ser suficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Precision

- Una Precisión de 1 indica que todas las predicciones positivas del modelo son correctas.
- Precisión cercana a 0 indica un alto número de falsos positivos.
- Es útil en contextos donde los falsos positivos son más costosos que los falsos negativos.





## Métricas: Sensibilidad

- La **Sensibilidad** (también conocida como **Recall** o **Tasa de Verdaderos Positivos**).

- Medida que evalúa la proporción de verdaderos positivos sobre el total de positivos.
    $$
    Sensibilidad = \frac{TP}{TP + FN}
    $$
    Donde:
    -   *TP* es el número de verdaderos positivos.
    -   *FN* es el número de falsos negativos.
    



    
## Métricas: Sensibilidad

- Métrica que evalúa el rendimiento del modelo en términos de **capacidad para identificar correctamente los positivos**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial identificar correctamente todos los casos positivos. (enfermedad)
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de aumentar los falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la tasa de falsos positivos**.
    :::
    Puede no ser suficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Sensibilidad

- Una Sensibilidad de 1 indica que el modelo identifica correctamente todos los positivos reales.
- Sensibilidad cercana a 0 indica que el modelo identifica incorrectamente la mayoría de los positivos reales.
- Es útil en contextos donde los falsos negativos son más costosos que los falsos positivos.



## Métricas: Especificidad

- La **Especificidad** (también conocida como **Tasa de Verdaderos Negativos**).

- Medida que evalúa la proporción de verdaderos negativos sobre el total de negativos.
    $$
    Especificidad = \frac{TN}{TN + FP}
    $$
    Donde:
    -   *TN* es el número de verdaderos negativos.
    -   *FP* es el número de falsos positivos.
    
## Métricas: Especificidad

- Métrica que evalúa el rendimiento del modelo en términos de **capacidad para identificar correctamente los negativos**.

-   ::: {style="color:#8ABF74;"}
    Importante en contextos donde es crucial identificar correctamente todos los casos negativos. (Inclusión)
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta a costa de aumentar los falsos negativos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la tasa de falsos negativos**.
    :::
    Insuficiente por sí sola para evaluar el rendimiento global del modelo.

## Métricas: Especificidad

- Una Especificidad de 1 indica que el modelo identifica correctamente todos los negativos reales.
- Especificidad cercana a 0 indica que el modelo identifica incorrectamente la mayoría de los negativos reales.
- Es útil en contextos donde los falsos positivos son más costosos que los falsos negativos.



## Métricas: J-Index

- El **Índice de Youden** (J-Index por sus siglas en inglés).

- Medida que combina la sensibilidad y la especificidad para evaluar el rendimiento general del modelo.
    $$
    J = Sensibilidad + Especificidad - 1
    $$
    Donde:
    -   *Sensibilidad* es la tasa de verdaderos positivos.
    -   *Especificidad* es la tasa de verdaderos negativos.
    
## Métricas: J-Index

- Métrica que evalúa el rendimiento del modelo en términos de **balance entre sensibilidad y especificidad**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida equilibrada del rendimiento del modelo considerando tanto los verdaderos positivos como los verdaderos negativos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - No pondera la importancia relativa de sensibilidad y especificidad, no distingue entre tipos de errores.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede no ser suficiente por sí solo**.
    :::
    Es útil en combinación con otras métricas para una evaluación más completa.

## Métricas: J-Index

- Un J-Index de 1 indica que el modelo predice perfectamente tanto los positivos como los negativos reales.
- J-Index cercano a 0 indica un bajo rendimiento del modelo.
- Es útil para comparar modelos en términos de su capacidad global para diferenciar entre clases.

## Métricas: F1 Score

- El **F1 Score**.

- Medida que combina la precisión y la sensibilidad en una única métrica armonizada.
    $$
    F1\ Score = 2 \times \frac{Precision \times Sensibilidad}{Precision + Sensibilidad}
    $$
    Donde:
    -   *Precision* es la precisión.
    -   *Sensibilidad* es la sensibilidad (también conocida como recall).
    
## Métricas: F1 Score

- Métrica que evalúa el rendimiento del modelo en términos de **balance entre precisión y sensibilidad**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida equilibrada del rendimiento del modelo cuando hay una distribución desigual de clases.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede no ser suficiente por sí sola en todos los contextos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede ser influenciada por clases desbalanceadas**.
    :::
    Es útil en combinación con otras métricas para una evaluación más completa.

## Métricas: F1 Score

- Un F1 Score de 1 indica que el modelo tiene perfecta precisión y sensibilidad.
- F1 Score cercano a 0 indica un bajo rendimiento del modelo.
- Es útil cuando se busca un equilibrio entre precisión y sensibilidad, especialmente en conjuntos de datos desbalanceados.




## Métricas: False Discovery Rate

- La **Tasa de Falsos Descubrimientos** (False Discovery Rate, FDR).

- Medida que evalúa la proporción de falsos positivos sobre el total de predicciones positivas realizadas por el modelo.
    $$
    FDR = \frac{FP}{TP + FP}
    $$
    Donde:
    -   *FP* es el número de falsos positivos.
    -   *TP* es el número de verdaderos positivos.
    
## Métricas: False Discovery Rate

- Métrica que evalúa el rendimiento del modelo en términos de **proporción de falsos positivos** en las predicciones positivas.

-   ::: {style="color:#8ABF74;"}
    Útil en contextos donde es crucial controlar el número de falsos positivos.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser alta si el modelo tiene baja precisión.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera los verdaderos negativos ni los falsos negativos**.
    :::
    Se enfoca solo en las predicciones positivas.

## Métricas: False Discovery Rate

- Una FDR de 0 indica que no hay falsos positivos en las predicciones positivas del modelo.
- FDR cercana a 1 indica que la mayoría de las predicciones positivas son incorrectas.
- Es útil en contextos donde los falsos positivos son más costosos y deben ser minimizados.




