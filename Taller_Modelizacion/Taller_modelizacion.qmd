---
title: "Modelizando"
format:
  revealjs:
    incremental: true  
    scrollable: true
    transition: slide
editor: 
  markdown: 
    wrap: 72
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
pacman::p_load(tidyverse)
```

# 1. Bases de modelización

## Bases de modelización

1.  Tener clara la pregunta
2.  Tener más o menos claras las variables que interfieren en el proceso
3.  Tener Una idea de la complejidad de la pregunta

# 2. Tipos de modelos

## Tipos de modelos

1.  Aprendizaje **supervisado**
2.  Aprendizaje **no supervisado**
3.  Aprendizaje **por refuerzo**

# 3. Aprendizaje supervisado

## Aprendizaje supervisado

"El aprendizaje supervisado es la tarea de crear una función que assigna
una entrada a una salida basándose en pares de entrada-salida de
ejemplo."

Kevin P. Murphy, *Machine Learning: A Probabilistic Perspective*

## Aprendizaje supervisado

-   ::: {style="color:#8ABF74;"}
    Relativamente, el tipo de modelo más fiable y conocido
    :::

-   ::: {style="color:#BF1E1A;"}
    Requiere de completa supervisión humana
    :::

-   ::: {style="color:#BF1E1A;"}
    Los datos pueden contener error humano, ergo los algoritmos aprendan
    incorrectamente
    :::

-   Tipos de modelos:

    -   **clasificación**: Variables factor.
    -   **regresion**: Variables numericas.

## Aprendizaje supervisado

::: columns
::: {.column width="50%"}
![](Imagenes/armas%20de%20destruccion%20matematica.jpg){width="328"}
:::

::: {.column width="50%"}
*"Los privilegiados son analizados por personas, las masas por
máquinas."*

**Cathy O'Neil**
:::
:::

## Aprendizaje supervisado

-   Educación: Despido de profesores en Washington DC y Chicago por dar
    demasiado peso en la fórmula las bajas calificaciones de los
    alumnos.
-   Justicia: Janet Collins y su esposo Malcolm a la cárcel por una probabilidad de 1 entre 12 millones. 
-   Política: microtargeting político en redes para dirigir campañas. 
-   Economía: sesgos en el crédito y trabajo de tipo espiral retroalimentativa. 


## Aprendizaje supervisado

1) Árboles de decisión
2) Naïve Bayes
3) Regresión
3) Técnicas de Supervivencia
5) Support Vector Machines (SVM)
6) SARIMA models
7) Redes neuronales



# 4. Aprendizaje no supervisado

## Aprendizaje no supervisado

-   Analiza y agrupa conjuntos de datos sin etiquetar.

-   Se basan en distancias. (Euclídeas o mahalanobis)

-   ::: {style="color:#8ABF74;"}
    Solución ideal para el análisis de datos exploratorios.
    :::

-   ::: {style="color:#BF1E1A;"}
    Computacionalmente duros
    :::

-   ::: {style="color:#BF1E1A;"}
    La interpretación puede ser un infierno
    :::

## Aprendizaje no supervisado

## Aprendizaje no supervisado: Agrupación en clústeres

-   La agrupación en clústeres de k-medias
-   Agrupación en clústeres jerárquica (HCA)
-   Agrupación en clústeres probabilística (GMM)

## Aprendizaje no supervisado: Reglas de asociación

*Identifican la probabilidad de consumir un producto dado el consumo de
otro producto*

## Aprendizaje no supervisado: Reducción de dimensionalidad

-   Análisis de componentes principales (PCA)

Este método utiliza una transformación lineal para crear una nueva
representación de datos

-   Descomposición en valores singulares

factoriza una matriz, A, en tres matrices de rango inferior.

## Aprendizaje no supervisado, caso Target

::: columns
::: {.column width="50%"}
![](Imagenes/target.png){width="359"}
:::

::: {.column width="50%"}
1.  Fundada en 1962
2.  Es la sexta empresa de venta al por menor más grande de Estados
    Unidos.
3.  351.000 empleados
:::
:::

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_embarazo.png)

## Aprendizaje no supervisado, caso Target

![](Imagenes/tickets_compra_dinero.png)

## Aprendizaje no supervisado, caso Target

```{r }
Target_ejemplo <- tibble(
  ClienteID = 1:6,
  VitaminasPrenatales = c(1, 0, 0, 0, 1, 0),
  LocionSinFragancia = c(1, 0, 0, 0, 1, 0),
  JabonSinFragancia = c(0, 1, 0, 0, 1, 0),
  Calcio = c(0, 1, 0, 0, 1, 1),
  Panales = c(0, 0, 1, 0, 0, 1),
  Toallitas = c(0, 0, 1, 0, 0, 0)
)

knitr::kable(Target_ejemplo)

```

## Aprendizaje no supervisado, caso Target

```{r echo=TRUE}
# Aplicar K-means
set.seed(123) # Para reproducibilidad
kmeans_result <- stats::kmeans(Target_ejemplo[,-1], centers = 3)

# Añadir los resultados del cluster al dataframe
Target_ejemplo <- Target_ejemplo %>%  
  mutate(Cluster = as_factor(kmeans_result$cluster) ) %>% 
  select(ClienteID, Cluster, everything())
```

## Aprendizaje no supervisado, caso Target

```{r}
knitr::kable(Target_ejemplo)
```

## Aprendizaje no supervisado, caso Target

```{r}
Target_ejemplo %>%  
  pivot_longer(
    cols= -c(Cluster,ClienteID),
    names_to =  'Producto',
    values_to = 'Compra_NoCompra') %>% 
  mutate(Compra_NoCompra = as_factor(Compra_NoCompra) ) %>% 
  ggplot(aes(x= Cluster, y= Producto, fill= Compra_NoCompra ) ) +
  geom_tile()
```

# 5. Aprendizaje por refuerzo

## 5 Aprendizaje por refuerzo

-   Entrena al software para que tome decisiones a fin de lograr los
    mejores resultados.

-   Ensayo y error (Supervisado y no supervisado al mismo tiempo)

## Aprendizaje por refuerzo

1.  Sobresale en entornos complejos

2.  Optimiza de acuerdo con objetivos a largo plazo

3.  Requiere menos interacción humana

## Aprendizaje por refuerzo

{{< video /Imagenes/Mario_IA_1.mp4 >}}

## Aprendizaje por refuerzo

{{< video /Imagenes/Mario_IA_2.mp4 >}}

## Aprendizaje por refuerzo

{{< video /Imagenes/Pokemon_IA_1.mp4 >}}

## Aprendizaje por refuerzo

{{< video /Imagenes/Pokemon_IA_2.mp4 >}}

## Aprendizaje por refuerzo

{{< video /Imagenes/Pokemon_IA_3.mp4 >}}

# Tipos de modelos 2

## Tipos de modelos 2

|                | **Modelos explicativos** | **Modelos predictivos** |
|----------------|:-------------------------|:------------------------|
| Relación x e y | Causalidad               | Asociación              |
| Protagonista   | Teoria                   | Datos                   |
| Visión         | Retrospectiva            | Futura                  |
| Varianza       | Minimizar sesgo          | Maximizar predicción    |

## Tipos de modelos 2

|                              | Modelos explicativos                                                             | Modelos predictivos                                                                                                                  |
|-------------------|---------------------------|--------------------------|
| Objetivos                    | Establecer relaciones causales                                                   | Predecir diagnósticos actuales o resultados futuros                                                                                  |
| Precaución                   | Las posibilidades de hallazgos (Error tipo I)                                    | No ajuste: falta de generalizabilidad a nuevas poblaciones                                                                           |
| Variables candidatas         | Un conjunto limitado de factores de riesgo y factores de confusión               | Muchos predictores potenciales; algunos pueden no tener relación causal con el resultado.                                            |
| Selección de variables       | Basada en hipótesis; No debe utilizar procedimientos de selección automatizados. | Exploratorio; Puede utilizar procedimientos de selección automatizados, siempre con la correspondiente validación.                   |
| Formas de ver el rendimiento | Tamaño de los coeficientes y nivel de significación de las variables             | Discriminación (análisis ROC); calibración (Hosmer-Lemeshow); de ajuste (R2, AIC); reclasificación (índice de reclasificación neta); |
| Validación                   | Se necesitan nuevos estudios para confirmar las relaciones causales individuales | Validación interna; muestreo dividido; validación cruzada; bootstrap; validación externa.                                            |

## Tipos de modelos 2

::: columns
::: {.column width="50%"}
Los investigadores con objetivos **explicativos** se desvían.

Intentan optimizar métricas del modelo; $R^2$,curvas ROC ..., y
descuidan cuestiones como la confusión.
:::

::: {.column width="50%"}
Los investigadores con objetivos **predictivos** se desvían.

preocupándose por los coeficientes $\beta$ y valores p, y omiten omiten
pasos críticos como la calibración o la validación.
:::
:::

## Tipos de modelos 2

![](Imagenes/Dilema.jpg)

## Tipos de modelos 2




# Metricas

## Métricas para modelos de respuesta Numérica

Toda métrica de evaluación para un modelo de machine learning supervisado se basa 

$$
e_{i} = original_{i} - prediccion_{i}
$$
$$
e_{i} = y_{i} - \hat{y_{i}}
$$

$$
e_{i} = f(y_{i} - \hat{y_{i}})
$$


## Métricas: MAE

- El **Error Absoluto Medio** (MAE por sus siglas en inglés *Mean Absolute Error*) 

- Medida de la magnitud promedio de los errores en un conjunto de predicciones, sin considerar su dirección.
    $$
    MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *\hat{y}_{i}* es el valor predicho por el modelo.
    
  
## Métricas: MAE

- Métrica que evalua el rendimiento del modelo en términos de la magnitud del error. Basada en la **diferencia absoluta promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los datos originales.
    :::

-   ::: {style="color:#BF1E1A;"}
    - No penaliza en exceso los valores atípicos (outliers) o errores extremos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::
    Trata todos los errores por igual, independientemente de si son positivos o negativos.


## Métricas: MAE

- Un MAE de 0 indica que el modelo predice exactamente los valores reales.
- $MAE  > 0$ indica que el modelo tiene un mayor error promedio.
- No penaliza ni discrimina entre errores grande o pequeños, es un promedio directo. 





## Métricas: MAPE

- El **Error Absoluto Medio Porcentual** (MAPE por sus siglas en inglés *Mean Absolute Percentage Error*).

- Medida de la magnitud promedio de los errores en un conjunto de predicciones, expresada como un porcentaje.
    $$
    MAPE = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *\(\hat{y}_i\)* es el valor predicho por el modelo.
    
## Métricas: MAPE

- Métrica que evalúa el rendimiento del modelo en términos de la magnitud del error relativo. Basada en la **diferencia porcentual promedio absoluta**.

-   ::: {style="color:#8ABF74;"}
    Expresa el error como un porcentaje, lo cual facilita la interpretación y comparación entre diferentes modelos y escalas.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser muy grande o indefinida si los valores observados son cercanos a cero.
    :::

-   ::: {style="color:#BF1E1A;"}
    **No considera la dirección de los errores**.
    :::
    Trata todos los errores por igual, independientemente de si son positivos o negativos.

## Métricas: MAPE

- Un MAPE de 0% indica que el modelo predice exactamente los valores reales.
- MAPE > 0% indica que el modelo tiene un mayor error porcentual promedio.
- Es útil para comparar la precisión de modelos en diferentes escalas y unidades, pero puede ser problemático con valores observados cercanos a cero.




## Métricas: MSE

- El **Error Cuadrático Medio** (MSE por sus siglas en inglés *Mean Squared Error*).

- Medida de la magnitud promedio de los errores en un conjunto de predicciones, considerando la dirección y penalizando los errores grandes más que los pequeños.
    $$
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *\(\hat{y}_i\)* es el valor predicho por el modelo.
    
## Métricas: MSE

- Métrica que evalúa el rendimiento del modelo en términos de la magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    - Penaliza los errores grandes más que los pequeños debido a la elevación al cuadrado de las diferencias.
    :::

-   ::: {style="color:#BF1E1A;"}
    Se utiliza principalmente en análisis teóricos y en optimización de modelos de alta complejidad. 
    :::
    - las unidades  del error son al cuadrado, puede ser difícil de interpretar directamente en términos de la escala original de los datos.

-   ::: {style="color:#BF1E1A;"}
    **Es sensible a los valores atípicos**.
    :::
    Unos pocos valores atípicos pueden incrementar significativamente el MSE.


## Métricas: MSE

- Un MSE de 0 indica que el modelo predice exactamente los valores reales.
- MSE > 0 indica que el modelo tiene un mayor error promedio.
- Penaliza más los errores grandes que los pequeños. Útil en contextos donde los errores grandes son especialmente perjudiciales.



## Métricas: RMSE

- El **Error Cuadrático Medio Raiz** (RMSE por sus siglas en inglés *Root Mean Squared Error*).

- Medida de la magnitud promedio de los errores en un conjunto de predicciones, considerando su dirección.
    $$
    RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *\(\hat{y}_i\)* es el valor predicho por el modelo.
    
## Métricas: RMSE

- Métrica que evalúa el rendimiento del modelo en términos de la magnitud del error. Basada en la **diferencia cuadrática promedio**.

-   ::: {style="color:#8ABF74;"}
    Fácil de interpretar ya que se encuentra en la misma escala que los datos originales.
    :::

- Penaliza altamente los valores atípicos (outliers) o errores extremos debido a la elevación al cuadrado de las diferencias y luego devolverlos a la escala original.


-   ::: {style="color:#BF1E1A;"}
    Es **muy** sensible a los valores atípicos.
    :::

## Métricas: RMSE

- Un RMSE de 0 indica que el modelo predice exactamente los valores reales.
- RMSE > 0 indica que el modelo tiene un mayor error promedio.
- Penaliza los errores grandes más que los pequeños, por lo tanto, es útil en contextos donde los errores grandes son especialmente perjudiciales.





## Métricas: $R^2$

- El **Coeficiente de Determinación** $R^2$.

- Medida que indica la proporción de la variabilidad de la variable dependiente que es explicada por el modelo.
    $$
    R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *$\hat{y}_{i}$* es el valor predicho por el modelo.
    -   *$\bar{y}$* es la media de los valores observados.

  
## Métricas: $R^2$

- Métrica que evalúa el rendimiento del modelo en términos de la proporción de variabilidad explicada. Basada en la **relación entre la suma de los errores residuales y la suma total de los cuadrados**.

-   ::: {style="color:#8ABF74;"}
    Facilita la interpretación de la efectividad del modelo, ya que varía entre 0 y 1.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser engañoso en modelos no lineales o con muchos parámetros(atención modelos predictivos)
    :::

-   ::: {style="color:#BF1E1A;"}
    **No informa sobre el sesgo o la varianza del modelo**.
    :::
    Solo indica la proporción de la varianza explicada.

## Métricas: $R^2$

- Un $R^2$ de 1 indica que el modelo explica toda la variabilidad de los datos.
- $R^2 = 0$ indica que el modelo no explica ninguna variabilidad de los datos.
- Valores negativos de $R^2$ pueden ocurrir y indican que el modelo es peor que uno que simplemente predice la media de los datos.





## Métricas: Huber Loss

- también conocida como **error cuadrático medio suavizado**

- Ccombina el MAE con MSE, siendo menos sensible a los valores atípicos que el MSE y más robusta que el MAE.
    $$
    L_\delta(y, \hat{y}) = 
    \begin{cases} 
    \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \le \delta \\
    \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{si } |y - \hat{y}| > \delta
    \end{cases}
    $$
    Donde:
    -   *y* es el valor observado.
    -   *$\hat{y}$* es el valor predicho por el modelo.
    -   *$\delta$* es un umbral positivo que determina la transición entre el MAE y el MSE.

## Métricas: Huber Loss

- Métrica que evalúa el rendimiento del modelo en términos de la magnitud del error, combinando **propiedades del MAE y el MSE**.

- La elección de $\delta$ es crucial y puede influir en la sensibilidad del modelo a los valores atípicos.

- Muy utlizado en técnicas de optimización basadas en gradientes. (XGBoost) 

## Métricas: Huber Loss

- Un valor de Huber Loss cercano a 0 indica que el modelo predice con alta precisión.
- La Huber Loss se ajusta dependiendo de la elección de $\delta$, lo que permite controlar el equilibrio entre penalización de errores grandes y pequeños.
- Resumen información del MAE i el MSE, de modo que vale la pena computar ambos para tener un contexto amplio de la situación. 






## Métricas: ICC

- El **Índice de Ideabilidad de Correlación** (ICC por sus siglas en inglés *Index of Ideality of Correlation*).

- Medida que evalúa la precisión predictiva de un modelo al comparar los valores predichos con los valores observados.
    $$
    ICC = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \overline{y})^2 + \sum_{i=1}^{n} (\hat{y}_i - \overline{\hat{y}})^2}
    $$
    Donde:
    -   *n* es el número de observaciones.
    -   *y<sub>i</sub>* es el valor observado.
    -   *\(\hat{y}_i\)* es el valor predicho por el modelo.
    -   *\(\overline{y}\)* es la media de los valores observados.
    -   *\(\overline{\hat{y}}\)* es la media de los valores predichos.

  
## Métricas: ICC

- Métrica que evalúa el rendimiento del modelo en términos de precisión predictiva. Basada en la **relación entre la suma de los errores residuales y las sumas de las desviaciones totales y predichas**.

-   ::: {style="color:#8ABF74;"}
    Proporciona una medida comprensible de la precisión predictiva del modelo.
    :::

-   ::: {style="color:#BF1E1A;"}
    - Puede ser sensible a la distribución de los valores observados y predichos.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Puede ser influenciado por la escala de los datos**.
    :::
    Los resultados pueden variar dependiendo de la escala de los valores observados y predichos.

## Métricas: ICC

- Un ICC de 1 indica que el modelo predice exactamente los valores reales.
- ICC cercano a 0 indica que el modelo no tiene una buena precisión predictiva.
- ICC puede ser negativo si el modelo tiene un rendimiento peor que el promedio.






## Métricas: AIC

-   El Criterio de Información de Akaike (Akaike 1974), AIC por sus
    siglas en inglés *Akaike Information Criterion*.

    $$
    AIC = 2{k} - \ln(\mathcal{L}) 
    $$

    Donde:

    -   *k* es el numero de parametros
    -   $\mathcal{L}$ es la verosimilitud del modelo.

## Métricas: AIC

-   Métrica basada en **teoría de información**, la cual balancea los
    poderes explicativos y predictivos del modelo

-   ::: {style="color:#8ABF74;"}
    Castiga el uso de muchas variables.
    :::

-   ::: {style="color:#BF1E1A;"}
    **Un AIC por si solo no tiene ningún valor**.
    :::
    Se necesitan de otras
    métricas o compararse con el AIC de otro modelo2 para darle
    contexto.
    

## Métricas: AIC

![](Imagenes/AIC%20vs%20R2.png){width="711" height="469"}

